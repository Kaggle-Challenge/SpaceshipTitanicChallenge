
# Import necessary libaries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline


import shap
import warnings
warnings.filterwarnings("ignore")

# Load Data
train = pd.read_csv('https://raw.githubusercontent.com/Revathi343664/SpaceshipTitanicChallenge/refs/heads/main/train.csv')
test = pd.read_csv('https://raw.githubusercontent.com/Revathi343664/SpaceshipTitanicChallenge/refs/heads/main/test.csv')
submission = pd.read_csv('https://raw.githubusercontent.com/Revathi343664/SpaceshipTitanicChallenge/refs/heads/main/sample_submission.csv')
test_ids = test['PassengerId']

# Missing Values Overview
print("Missing values in train:\n", train.isnull().sum())

# Initial EDA
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
sns.histplot(train['Age'].dropna(), kde=True, ax=axes[0]).set(title='Age Distribution')
sns.countplot(x='VIP', data=train, ax=axes[1]).set(title='VIP Count')
sns.countplot(x='CryoSleep', data=train, ax=axes[2]).set(title='CryoSleep Count')
plt.tight_layout()
plt.show()

# Cabin Feature Engineering
for df in [train, test]:
    df[['Deck','CabinNum','Side']] = df['Cabin'].str.split('/', expand=True)
    df['CabinNum'] = pd.to_numeric(df['CabinNum'], errors='coerce')

# Missing Value Imputation
cols_cat = ['HomePlanet','CryoSleep','Destination','VIP','Deck','Side']
cols_num = ['Age','RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','CabinNum']

for df in [train, test]:
    for c in cols_cat:
        df[c].fillna(df[c].mode()[0], inplace=True)
    for c in cols_num:
        df[c].fillna(df[c].median(), inplace=True)

# Feature Engineering
for df in [train, test]:
    df['TotalExpenses'] = df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck']].sum(axis=1)
# Encoding + Scaling
le = LabelEncoder()
for c in cols_cat:
    train[c] = le.fit_transform(train[c].astype(str))
    test[c] = le.transform(test[c].astype(str))

scaler = StandardScaler()
train[cols_num + ['TotalExpenses']] = scaler.fit_transform(train[cols_num + ['TotalExpenses']])
test[cols_num + ['TotalExpenses']] = scaler.transform(test[cols_num + ['TotalExpenses']])

# Encoding + Scaling
le = LabelEncoder()
for c in cols_cat:
    train[c] = le.fit_transform(train[c].astype(str))
    test[c] = le.transform(test[c].astype(str))

scaler = StandardScaler()
train[cols_num + ['TotalExpenses']] = scaler.fit_transform(train[cols_num + ['TotalExpenses']])
test[cols_num + ['TotalExpenses']] = scaler.transform(test[cols_num + ['TotalExpenses']])

# Data Preparation for Modeling
X = train.drop(['PassengerId','Name','Cabin','Transported'], axis=1)
y = train['Transported'].astype(int)
X_test = test.drop(['PassengerId','Name','Cabin'], axis=1)

# Enhanced EDA

# Correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(train[cols_num + ['TotalExpenses'] + ['Transported']].corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# Scatter Plots
plt.figure(figsize=(8,6))
sns.scatterplot(x='Age', y='TotalExpenses', hue='Transported', data=train)
plt.title('Age vs TotalExpenses')
plt.show()

plt.figure(figsize=(8,6))
sns.scatterplot(x='Spa', y='VRDeck', hue='Transported', data=train)
plt.title('Spa vs VRDeck')
plt.show()

# Pairplot
sns.pairplot(train[['Age', 'RoomService', 'VRDeck', 'Spa', 'TotalExpenses', 'Transported']].dropna(), hue='Transported', diag_kind='kde')
plt.suptitle("Pairplot of Key Features", y=1.02)
plt.show()

# Categorical comparisons
for col in ['VIP', 'Destination', 'Deck']:
    plt.figure(figsize=(6,4))
    sns.countplot(x=col, hue='Transported', data=train)
    plt.title(f'{col} vs Transported')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# Let's decide how many components to keep. We can visualize explained variance.
pca = PCA().fit(X)
plt.figure(figsize=(10, 6))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA - Cumulative Explained Variance')
plt.grid(True)
plt.show()

# Based on the plot, we have choosen 10 number of components that explain a good portion of variance nearly 95% because of typical dataset.
# We have adjusted `n_components` based on the Elbow point or desired explained variance from the plot above.
pca_n_components = min(10, X.shape[1])

pca_transformer = PCA(n_components=pca_n_components)
X_pca = pca_transformer.fit_transform(X)
X_test_pca = pca_transformer.transform(X_test)

print(f"\nOriginal number of features: {X.shape[1]}")
print(f"Number of features after PCA: {X_pca.shape[1]}")

# --- 10. Model Training with Hyperparameter Tuning and Cross-Validation ---

# Creating StratifiedKFold for robust evaluation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# XGBoost Classifier
print("\n--- Training XGBoost Classifier with Hyperparameter Tuning ---")

# Define parameter grid for RandomizedSearchCV (more efficient than GridSearchCV for large spaces)
# For a full GridSearchCV, define a smaller, more focused grid after initial exploration.
xgb_param_grid = {
    'n_estimators': [100, 200, 300, 400],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 9],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3],
    'lambda': [0.1, 1, 5], # L2 regularization term
    'alpha': [0, 0.1, 0.5] # L1 regularization term
}

# Use RandomizedSearchCV for faster initial exploration
xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_random_search = RandomizedSearchCV(xgb_model, xgb_param_grid, n_iter=30, cv=skf,
                                       scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)
# Using original features for tuning initially. You can also tune on PCA features.
xgb_random_search.fit(X, y)

print(f"Best XGBoost parameters: {xgb_random_search.best_params_}")
print(f"Best XGBoost cross-validation accuracy: {xgb_random_search.best_score_:.4f}")

best_xgb_model = xgb_random_search.best_estimator_

# LightGBM Classifier
print("\n--- Training LightGBM Classifier with Hyperparameter Tuning ---")

lgbm_param_grid = {
    'n_estimators': [100, 200, 300, 400],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'num_leaves': [20, 31, 40, 50],
    'max_depth': [-1, 5, 7, 9], # -1 means no limit
    'min_child_samples': [20, 30, 40],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
    'reg_alpha': [0, 0.1, 0.5], # L1 regularization
    'reg_lambda': [0.1, 1, 5] # L2 regularization
}

lgbm_model = LGBMClassifier(random_state=42)
lgbm_random_search = RandomizedSearchCV(lgbm_model, lgbm_param_grid, n_iter=30, cv=skf,
                                        scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)
# Using original features for tuning
lgbm_random_search.fit(X, y)

print(f"Best LightGBM parameters: {lgbm_random_search.best_params_}")
print(f"Best LightGBM cross-validation accuracy: {lgbm_random_search.best_score_:.4f}")

best_lgbm_model = lgbm_random_search.best_estimator_

# --- LightGBM Classifier ---
print("\n--- Training LightGBM Classifier with Hyperparameter Tuning ---")

lgbm_param_grid = {
    'n_estimators': [100, 200, 300, 400],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'num_leaves': [20, 31, 40, 50],
    'max_depth': [-1, 5, 7, 9], # -1 means no limit
    'min_child_samples': [20, 30, 40],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
    'reg_alpha': [0, 0.1, 0.5], # L1 regularization
    'reg_lambda': [0.1, 1, 5] # L2 regularization
}

lgbm_model = LGBMClassifier(random_state=42)
lgbm_random_search = RandomizedSearchCV(lgbm_model, lgbm_param_grid, n_iter=50, cv=skf,
                                        scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)
# Using original features for tuning
lgbm_random_search.fit(X, y)

print(f"Best LightGBM parameters: {lgbm_random_search.best_params_}")
print(f"Best LightGBM cross-validation accuracy: {lgbm_random_search.best_score_:.4f}")

best_lgbm_model = lgbm_random_search.best_estimator_

# CatBoost Classifier
print("\n--- Training CatBoost Classifier with Hyperparameter Tuning ---")
catboost_params = {
    'iterations': [100, 300, 500],
    'learning_rate': [0.03, 0.1, 0.2],
    'depth': [4, 6, 8, 10],
    'l2_leaf_reg': [1, 3, 5, 7]
}

cat_model = CatBoostClassifier(verbose=0, random_state=42)
cat_search = RandomizedSearchCV(cat_model, catboost_params, n_iter=30, cv=skf, scoring='accuracy', random_state=42, n_jobs=-1, verbose=1)
cat_search.fit(X, y)

print("Best CatBoost parameters:", cat_search.best_params_)
print(f"Best CatBoost CV accuracy: {cat_search.best_score_:.4f}")
best_cat_model = cat_search.best_estimator_

# Random Forest Classifier
print("\n--- Training Random Forest Classifier with Hyperparameter Tuning ---")
rf_param_grid = {
    'n_estimators': [100, 200, 300, 400],
    'max_depth': [None, 5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

rf_model = RandomForestClassifier(random_state=42)
rf_search = RandomizedSearchCV(rf_model, rf_param_grid, n_iter=30, cv=skf, scoring='accuracy', n_jobs=-1, verbose=1, random_state=42)
rf_search.fit(X, y)

print("Best Random Forest parameters:", rf_search.best_params_)
print(f"Best Random Forest CV accuracy: {rf_search.best_score_:.4f}")
best_rf_model = rf_search.best_estimator_
# Cross-validation accuracies
print(f"Best XGBoost cross-validation accuracy: {xgb_random_search.best_score_:.4f}")
print(f"Best LightGBM cross-validation accuracy: {lgbm_random_search.best_score_:.4f}")
print(f"Best CatBoost cross-validation accuracy: {cat_search.best_score_:.4f}")
print(f"Best Random Forest cross-validation accuracy: {rf_search.best_score_:.4f}")

